<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
							<email>alekha@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT</orgName>
								<orgName type="department" key="dep2">Dept. of EECS and Statistics UC Berkeley</orgName>
								<orgName type="institution">Microsoft Research New York NY</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><forename type="middle">N</forename><surname>Negahban</surname></persName>
							<email>sahandn@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT</orgName>
								<orgName type="department" key="dep2">Dept. of EECS and Statistics UC Berkeley</orgName>
								<orgName type="institution">Microsoft Research New York NY</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
							<email>wainwrig@stat.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT</orgName>
								<orgName type="department" key="dep2">Dept. of EECS and Statistics UC Berkeley</orgName>
								<orgName type="institution">Microsoft Research New York NY</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a O(d/T) convergence rate for strongly convex objectives in d dimensions and O(s(log d)/T) convergence rate when the optimum is s-sparse. Our algorithm is based on successively solving a series of â„“ 1-regularized optimization problems using Nesterov&apos;s dual averaging algorithm. We establish that the error of our solution after T iterations is at most O(s(log d)/T), with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A multi-agent reinforcement learning model of common-pool resource appropriation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perolat</surname></persName>
							<email>perolat@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeepMind London</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<country>UK, UK, UK, UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeepMind London</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<country>UK, UK, UK, UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
							<email>vzambaldi@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeepMind London</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<country>UK, UK, UK, UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
							<email>cbeattie@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeepMind London</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<country>UK, UK, UK, UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
							<email>karltuyls@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeepMind London</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<country>UK, UK, UK, UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeepMind London</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<country>UK, UK, UK, UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A multi-agent reinforcement learning model of common-pool resource appropriation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria-a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Higher Order Statistical Decorrelation without Information Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Deco</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Technische UniversiUit MUnchen Institut fur InfoIIDatik Arcisstr. 21</orgName>
								<orgName type="institution">SiemensAG Central Research Otto-Hahn-Ring</orgName>
								<address>
									<postCode>6 81739</postCode>
									<settlement>Munich GeIIDany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Brauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Technische UniversiUit MUnchen Institut fur InfoIIDatik Arcisstr. 21</orgName>
								<orgName type="institution">SiemensAG Central Research Otto-Hahn-Ring</orgName>
								<address>
									<postCode>6 81739</postCode>
									<settlement>Munich GeIIDany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Deco</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Technische UniversiUit MUnchen Institut fur InfoIIDatik Arcisstr. 21</orgName>
								<orgName type="institution">SiemensAG Central Research Otto-Hahn-Ring</orgName>
								<address>
									<postCode>6 81739</postCode>
									<settlement>Munich GeIIDany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Brauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Technische UniversiUit MUnchen Institut fur InfoIIDatik Arcisstr. 21</orgName>
								<orgName type="institution">SiemensAG Central Research Otto-Hahn-Ring</orgName>
								<address>
									<postCode>6 81739</postCode>
									<settlement>Munich GeIIDany</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Higher Order Statistical Decorrelation without Information Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>248</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>80290 Munich GeIIDany A neural network learning paradigm based on information theory is proposed as a way to perform in an unsupervised fashion, redundancy reduction among the elements of the output layer without loss of information from the sensory input. The model developed performs nonlin-ear decorrelation up to higher orders of the cumulant tensors and results in probabilistic ally independent components of the output layer. This means that we don&apos;t need to assume Gaussian distribution neither at the input nor at the output. The theory presented is related to the unsuper-vised-learning theory of Barlow, which proposes redundancy reduction as the goal of cognition. When nonlinear units are used nonlinear principal component analysis is obtained. In this case nonlinear manifolds can be reduced to minimum dimension manifolds. If such units are used the network performs a generalized principal component analysis in the sense that non-Gaussian distributions can be linearly decorrelated and higher orders of the correlation tensors are also taken into account. The basic structure of the architecture involves a general transfOlmation that is volume conserving and therefore the entropy, yielding a map without loss of infoIIDation. Minimization of the mutual infoIIDation among the output neurons eliminates the redundancy between the outputs and results in statistical decorrelation of the extracted features. This is known as factorialleaming.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

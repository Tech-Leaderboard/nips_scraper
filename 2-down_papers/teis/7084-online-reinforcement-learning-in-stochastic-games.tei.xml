<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Reinforcement Learning in Stochastic Games</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science Academia Sinica</orgName>
								<orgName type="department" key="dep2">Institute of Information Science Academia Sinica</orgName>
								<orgName type="department" key="dep3">Institute of Information Science Academia Sinica</orgName>
								<address>
									<country>Taiwan, Taiwan, Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Te</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science Academia Sinica</orgName>
								<orgName type="department" key="dep2">Institute of Information Science Academia Sinica</orgName>
								<orgName type="department" key="dep3">Institute of Information Science Academia Sinica</orgName>
								<address>
									<country>Taiwan, Taiwan, Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Jen</forename><surname>Lu</surname></persName>
							<email>cjlu@iis.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science Academia Sinica</orgName>
								<orgName type="department" key="dep2">Institute of Information Science Academia Sinica</orgName>
								<orgName type="department" key="dep3">Institute of Information Science Academia Sinica</orgName>
								<address>
									<country>Taiwan, Taiwan, Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Reinforcement Learning in Stochastic Games</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the UCSG algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the diameter, which is an intrinsic value related to the mixing property of SGs. If we let the opponent play an optimistic best response to the learner, UCSG finds an ε-maximin stationary policy with a sample complexity of˜O of˜ of˜O (poly(1/ε)), where ε is the gap to the best policy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

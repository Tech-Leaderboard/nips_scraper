<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tempering Backpropagation Networks: Not All Weights are Created Equal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Neurobiology Lab The Salk Institute for BioI. Studies San Diego</orgName>
								<orgName type="institution">EVOTEC BioSystems GmbH</orgName>
								<address>
									<addrLine>Grandweg 64</addrLine>
									<postCode>22529, 92186-5800</postCode>
									<settlement>Hamburg</settlement>
									<region>CA</region>
									<country>Germany, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Neurobiology Lab The Salk Institute for BioI. Studies San Diego</orgName>
								<orgName type="institution">EVOTEC BioSystems GmbH</orgName>
								<address>
									<addrLine>Grandweg 64</addrLine>
									<postCode>22529, 92186-5800</postCode>
									<settlement>Hamburg</settlement>
									<region>CA</region>
									<country>Germany, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tempering Backpropagation Networks: Not All Weights are Created Equal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Backpropagation learning algorithms typically collapse the network&apos;s structure into a single vector of weight parameters to be optimized. We suggest that their performance may be improved by utilizing the structural information instead of discarding it, and introduce a framework for &apos;&apos;tempering&apos;&apos; each weight accordingly. In the tempering model, activation and error signals are treated as approximately independent random variables. The characteristic scale of weight changes is then matched to that ofthe residuals, allowing structural properties such as a node&apos;s fan-in and fan-out to affect the local learning rate and backpropagated error. The model also permits calculation of an upper bound on the global learning rate for batch updates, which in turn leads to different update rules for bias vs. non-bias weights. This approach yields hitherto unparalleled performance on the family relations benchmark, a deep multi-layer network: for both batch learning with momentum and the delta-bar-delta algorithm, convergence at the optimal learning rate is sped up by more than an order of magnitude.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
							<email>crussell@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The Alan Turing Institute and University College London</orgName>
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Surrey</orgName>
								<orgName type="institution" key="instit2">University of Warwick</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
							<email>mkusner@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The Alan Turing Institute and University College London</orgName>
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Surrey</orgName>
								<orgName type="institution" key="instit2">University of Warwick</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
							<email>loftus@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The Alan Turing Institute and University College London</orgName>
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Surrey</orgName>
								<orgName type="institution" key="instit2">University of Warwick</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
							<email>ricardo@stats.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The Alan Turing Institute and University College London</orgName>
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Surrey</orgName>
								<orgName type="institution" key="instit2">University of Warwick</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine learning is now being used to make crucial decisions about people&apos;s lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from coun-terfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal &quot;world&quot; is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

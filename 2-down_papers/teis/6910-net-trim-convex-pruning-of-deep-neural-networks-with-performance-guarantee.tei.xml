<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Aghasi</surname></persName>
							<email>aaghasi@gsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Insight</orgName>
								<orgName type="department" key="dep2">Department of ECE</orgName>
								<orgName type="department" key="dep3">Department of ECE</orgName>
								<orgName type="institution" key="instit1">Georgia State University</orgName>
								<orgName type="institution" key="instit2">IBM TJ Watson</orgName>
								<orgName type="institution" key="instit3">IBM TJ Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Abdi</surname></persName>
							<email>abdi@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Insight</orgName>
								<orgName type="department" key="dep2">Department of ECE</orgName>
								<orgName type="department" key="dep3">Department of ECE</orgName>
								<orgName type="institution" key="instit1">Georgia State University</orgName>
								<orgName type="institution" key="instit2">IBM TJ Watson</orgName>
								<orgName type="institution" key="instit3">IBM TJ Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Insight</orgName>
								<orgName type="department" key="dep2">Department of ECE</orgName>
								<orgName type="department" key="dep3">Department of ECE</orgName>
								<orgName type="institution" key="instit1">Georgia State University</orgName>
								<orgName type="institution" key="instit2">IBM TJ Watson</orgName>
								<orgName type="institution" key="instit3">IBM TJ Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Nguyen</surname></persName>
							<email>nnguyen@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Insight</orgName>
								<orgName type="department" key="dep2">Department of ECE</orgName>
								<orgName type="department" key="dep3">Department of ECE</orgName>
								<orgName type="institution" key="instit1">Georgia State University</orgName>
								<orgName type="institution" key="instit2">IBM TJ Watson</orgName>
								<orgName type="institution" key="instit3">IBM TJ Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Romberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Insight</orgName>
								<orgName type="department" key="dep2">Department of ECE</orgName>
								<orgName type="department" key="dep3">Department of ECE</orgName>
								<orgName type="institution" key="instit1">Georgia State University</orgName>
								<orgName type="institution" key="instit2">IBM TJ Watson</orgName>
								<orgName type="institution" key="instit3">IBM TJ Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Insight</orgName>
								<orgName type="department" key="dep2">Department of ECE</orgName>
								<orgName type="department" key="dep3">Department of ECE</orgName>
								<orgName type="institution" key="instit1">Georgia State University</orgName>
								<orgName type="institution" key="instit2">IBM TJ Watson</orgName>
								<orgName type="institution" key="instit3">IBM TJ Watson</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length N as inputs, we show that if the network response can be described using a maximum number of s non-zero weights per node, these weights can be learned from O(s log N) samples.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning curves for stochastic gradient descent in linear feedforward networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Werfel</surname></persName>
							<email>jkwerfel@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT Cambridge</orgName>
								<orgName type="department" key="dep2">Dept. of Molecular Biology Princeton University Princeton</orgName>
								<orgName type="department" key="dep3">Dept. of Brain &amp; Cog. Sci. MIT Cambridge</orgName>
								<address>
									<postCode>02139, 08544, 02139</postCode>
									<region>MA, NJ, MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT Cambridge</orgName>
								<orgName type="department" key="dep2">Dept. of Molecular Biology Princeton University Princeton</orgName>
								<orgName type="department" key="dep3">Dept. of Brain &amp; Cog. Sci. MIT Cambridge</orgName>
								<address>
									<postCode>02139, 08544, 02139</postCode>
									<region>MA, NJ, MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sebastian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT Cambridge</orgName>
								<orgName type="department" key="dep2">Dept. of Molecular Biology Princeton University Princeton</orgName>
								<orgName type="department" key="dep3">Dept. of Brain &amp; Cog. Sci. MIT Cambridge</orgName>
								<address>
									<postCode>02139, 08544, 02139</postCode>
									<region>MA, NJ, MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Hhmi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of EECS MIT Cambridge</orgName>
								<orgName type="department" key="dep2">Dept. of Molecular Biology Princeton University Princeton</orgName>
								<orgName type="department" key="dep3">Dept. of Brain &amp; Cog. Sci. MIT Cambridge</orgName>
								<address>
									<postCode>02139, 08544, 02139</postCode>
									<region>MA, NJ, MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning curves for stochastic gradient descent in linear feedforward networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difficulties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system ; with sufficiently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

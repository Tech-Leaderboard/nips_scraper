<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counterfactual Fairness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
							<email>mkusner@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Warwick</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
								<orgName type="institution" key="instit4">The Alan Turing Institute and University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Loftus</surname></persName>
							<email>loftus@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Warwick</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
								<orgName type="institution" key="instit4">The Alan Turing Institute and University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
							<email>crussell@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Warwick</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
								<orgName type="institution" key="instit4">The Alan Turing Institute and University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
							<email>ricardo@stats.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Alan Turing Institute and University of Warwick</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Surrey</orgName>
								<orgName type="institution" key="instit4">The Alan Turing Institute and University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Counterfactual Fairness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

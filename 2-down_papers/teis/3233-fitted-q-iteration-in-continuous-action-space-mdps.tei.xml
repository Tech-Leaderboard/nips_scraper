<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fitted Q-iteration in continuous action-space MDPs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1111">1111</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Antos</surname></persName>
							<email>antos@sztaki.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer and Automation Research Inst</orgName>
								<orgName type="institution">Hungarian Academy of Sciences Kende</orgName>
								<address>
									<postCode>1111</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
							<email>remi.munos@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">INRIA Lille</orgName>
								<address>
									<addrLine>59650 Villeneuve d&apos;Ascq</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
							<email>szepesva@cs.ualberta.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<addrLine>Edmonton T6G 2E8</addrLine>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fitted Q-iteration in continuous action-space MDPs</title>
					</analytic>
					<monogr>
						<title level="m">Computer and Automation Research Inst. of the Hungarian Academy of Sciences Kende u. 13-17</title>
						<meeting> <address><addrLine>Budapest; Hungary</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="1111">1111</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by some policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous analysis of this algorithm, proving what we believe is the first finite-time bound for value-function based algorithms for continuous state and action problems. 1 Preliminaries We will build on the results from [1, 2, 3] and for this reason we use the same notation as these papers. The unattributed results cited in this section can be found in the book [4]. A discounted MDP is defined by a quintuple (X , A, P, S, γ), where X is the (possible infinite) state space, A is the set of actions, P : X × A → M (X) is the transition probability kernel with P (·|x, a) defining the next-state distribution upon taking action a from state x, S(·|x, a) gives the corresponding distribution of immediate rewards, and γ ∈ (0, 1) is the discount factor. Here X is a measurable space and M (X) denotes the set of all probability measures over X. The Lebesgue-measure shall be denoted by λ. We start with the following mild assumption on the MDP: Assumption A1 (MDP Regularity) X is a compact subset of the d X-dimensional Euclidean space, A is a compact subset of [−A ∞ , A ∞ ] d A. The random immediate rewards are bounded byˆRbyˆ byˆR max and that the expected immediate reward function, r(x, a) = 񮽙 rS(dr|x, a), is uniformly bounded by R max : 񮽙r񮽙 ∞ ≤ R max. A policy determines the next action given the past observations. Here we shall deal with stationary (Markovian) policies which choose an action in a stochastic way based on the last observation only. The value of a policy π when it is started from a state x is defined as the total expected discounted reward that is encountered while the policy is executed: V π (x) = E π [ 񮽙 ∞ t=0 γ t R t |X 0 = x]. Here R t ∼ S(·|X t , A t) is the reward received at time step t, the state, X t , evolves according to X t+1 ∼ *</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

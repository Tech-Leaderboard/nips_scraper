<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallelizing MCMC with Random Partition Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Duke University</orgName>
								<orgName type="department" key="dep3">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep4">Dept. of Statistical Science Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangjian</forename><surname>Guo</surname></persName>
							<email>guo@cs.duke.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Duke University</orgName>
								<orgName type="department" key="dep3">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep4">Dept. of Statistical Science Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
							<email>kheller@stat.duke.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Duke University</orgName>
								<orgName type="department" key="dep3">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep4">Dept. of Statistical Science Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
							<email>dunson@stat.duke.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Duke University</orgName>
								<orgName type="department" key="dep3">Dept. of Statistical Science Duke University</orgName>
								<orgName type="department" key="dep4">Dept. of Statistical Science Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parallelizing MCMC with Random Partition Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to re-sample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

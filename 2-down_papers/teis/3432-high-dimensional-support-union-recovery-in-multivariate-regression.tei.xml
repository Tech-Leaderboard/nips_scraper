<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-dimensional support union recovery in multivariate regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics UC Berkeley</orgName>
								<orgName type="department" key="dep2">Department of Statistics Dept. of Electrical Engineering and Computer Science UC Berkeley</orgName>
								<orgName type="department" key="dep3">Department of Statistics Department of Electrical Engineering and Computer Science UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
							<email>wainwright@stat.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics UC Berkeley</orgName>
								<orgName type="department" key="dep2">Department of Statistics Dept. of Electrical Engineering and Computer Science UC Berkeley</orgName>
								<orgName type="department" key="dep3">Department of Statistics Department of Electrical Engineering and Computer Science UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@stat.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics UC Berkeley</orgName>
								<orgName type="department" key="dep2">Department of Statistics Dept. of Electrical Engineering and Computer Science UC Berkeley</orgName>
								<orgName type="department" key="dep3">Department of Statistics Department of Electrical Engineering and Computer Science UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High-dimensional support union recovery in multivariate regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the behavior of block 񮽙 1 // 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a fixed set of p co-variates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to infinity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 񮽙1//2 (n, p, s) : = n/[2ψ(B *) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B *) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. This sparsity-overlap function reveals that block 񮽙 1 // 2 regularization for multivariate regression never harms performance relative to a naive 񮽙 1-approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
							<email>hsedghi@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Univ. of Southern California Los Angeles</orgName>
								<orgName type="institution" key="instit1">Univ. of Southern California Los Angeles</orgName>
								<orgName type="institution" key="instit2">University of California Irvine</orgName>
								<address>
									<postCode>90089, 92697, 90089</postCode>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
							<email>a.anandkumar@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Univ. of Southern California Los Angeles</orgName>
								<orgName type="institution" key="instit1">Univ. of Southern California Los Angeles</orgName>
								<orgName type="institution" key="instit2">University of California Irvine</orgName>
								<address>
									<postCode>90089, 92697, 90089</postCode>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Jonckheere</surname></persName>
							<email>jonckhee@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Univ. of Southern California Los Angeles</orgName>
								<orgName type="institution" key="instit1">Univ. of Southern California Los Angeles</orgName>
								<orgName type="institution" key="instit2">University of California Irvine</orgName>
								<address>
									<postCode>90089, 92697, 90089</postCode>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of O(s log d/T) for s-sparse problems in d dimensions in T steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish O(1/T) rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>White</surname></persName>
							<email>whitem@cs.ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computing Science</orgName>
								<orgName type="department" key="dep2">Department of Computing Science</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">University of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>White</surname></persName>
							<email>awhite@cs.ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computing Science</orgName>
								<orgName type="department" key="dep2">Department of Computing Science</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">University of Alberta</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The reinforcement learning community has explored many approaches to obtaining value estimates and models to guide decision making; these approaches, however , do not usually provide a measure of confidence in the estimate. Accurate estimates of an agent&apos;s confidence are useful for many applications, such as biasing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing confidence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agent-environment interaction rarely satisfies traditional assumptions. Samples of value-estimates are dependent, likely non-normally distributed and often limited, particularly in early learning when confidence estimates are pivotal. In this work, we investigate how to compute robust confidences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applicability of our confidence estimation algorithms with experiments on exploration, parameter estimation and tracking.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

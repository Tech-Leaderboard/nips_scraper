<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cobot: A Social Reinforcement Learning Agent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Lee</forename><surname>Isbell</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Pennsylvania Syntek Capital AT&amp;T Labs-Research</orgName>
								<orgName type="institution">AT&amp;T Labs-Research Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Christian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Pennsylvania Syntek Capital AT&amp;T Labs-Research</orgName>
								<orgName type="institution">AT&amp;T Labs-Research Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shelton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Pennsylvania Syntek Capital AT&amp;T Labs-Research</orgName>
								<orgName type="institution">AT&amp;T Labs-Research Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Pennsylvania Syntek Capital AT&amp;T Labs-Research</orgName>
								<orgName type="institution">AT&amp;T Labs-Research Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Pennsylvania Syntek Capital AT&amp;T Labs-Research</orgName>
								<orgName type="institution">AT&amp;T Labs-Research Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Pennsylvania Syntek Capital AT&amp;T Labs-Research</orgName>
								<orgName type="institution">AT&amp;T Labs-Research Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cobot: A Social Reinforcement Learning Agent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modifing his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

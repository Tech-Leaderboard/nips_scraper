<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experts in a Markov Decision Process</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Even-Dar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">Computer Science Tel-Aviv University</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit3">Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
							<email>skakade@linc.cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">Computer Science Tel-Aviv University</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit3">Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
							<email>mansour@post.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">Computer Science Tel-Aviv University</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit3">Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Experts in a Markov Decision Process</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain fixed. Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time. We provide efficient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions. We also show that in the case that the dynamics change over time, the problem becomes computationally hard.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BinaryConnect: Training Deep Neural Networks with binary weights during propagations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariauxécole</surname></persName>
							<email>matthieu.courbariaux@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Jean-Pierre DavidÉcole David´DavidÉcole Polytechnique de Montréal</orgName>
								<orgName type="institution" key="instit1">Courbariaux´CourbariauxÉcole Polytechnique de Montréal</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<orgName type="institution" key="instit3">CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Jean-Pierre DavidÉcole David´DavidÉcole Polytechnique de Montréal</orgName>
								<orgName type="institution" key="instit1">Courbariaux´CourbariauxÉcole Polytechnique de Montréal</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<orgName type="institution" key="instit3">CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BinaryConnect: Training Deep Neural Networks with binary weights during propagations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g.-1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Performance of Convex Set projection Based Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Marks Ii</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Interactive Systems Design Lab</orgName>
								<orgName type="department" key="dep2">FT-IO</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>Wa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Interactive Systems Design Lab</orgName>
								<orgName type="department" key="dep2">FT-IO</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>Wa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seho</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Interactive Systems Design Lab</orgName>
								<orgName type="department" key="dep2">FT-IO</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>Wa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Ritcey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Interactive Systems Design Lab</orgName>
								<orgName type="department" key="dep2">FT-IO</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>Wa</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Performance of Convex Set projection Based Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>534</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We donsider a class of neural networks whose performance can be analyzed and geometrically visualized in a signal space environment. Alternating projection neural networks (APNN&apos; s) perform by alternately projecting between two or more constraint sets. Criteria for desired and unique convergence are easily established. The network can be configured in either a homogeneous or layered form. The number of patterns that can be stored in the network is on the order of the number of input and hidden neurons. If the output neurons can take on only one of two states, then the trained layered APNN can be easily configured to converge in one iteration. More generally, convergence is at an exponential rate. Convergence can be improved by the use of sigmoid type nonlinearities, network relaxation and/or increasing the number of neurons in the hidden layer. The manner in which the network responds to data for which it was not specifically trained (i.e. how it generalizes) can be directly evaluated analytically.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science</orgName>
								<orgName type="department" key="dep2">School of Science</orgName>
								<orgName type="laboratory">Academy of Mathematics and Systems Science Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit1">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit4">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science</orgName>
								<orgName type="department" key="dep2">School of Science</orgName>
								<orgName type="laboratory">Academy of Mathematics and Systems Science Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit1">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit4">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Liu</surname></persName>
							<email>ytliu@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science</orgName>
								<orgName type="department" key="dep2">School of Science</orgName>
								<orgName type="laboratory">Academy of Mathematics and Systems Science Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit1">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit4">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Ming</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science</orgName>
								<orgName type="department" key="dep2">School of Science</orgName>
								<orgName type="laboratory">Academy of Mathematics and Systems Science Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit1">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit4">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science</orgName>
								<orgName type="department" key="dep2">School of Science</orgName>
								<orgName type="laboratory">Academy of Mathematics and Systems Science Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit1">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Beijing Jiaotong University</orgName>
								<orgName type="institution" key="instit4">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In reinforcement learning (RL) , one of the key components is policy evaluation, which aims to estimate the value function (i.e., expected long-term accumulated reward) of a policy. With a good policy evaluation method, the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous Gradient-based Temporal Difference(GTD) policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming, a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated, previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting them into convex-concave saddle point problems. However, it is well-known that, the data are generated from Markov processes rather than i.i.d. in RL problems.. In this paper, in the realistic Markov setting, we derive the finite sample bounds for the general convex-concave saddle point problems, and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size, GTD algorithms converge. (2) The convergence rate is determined by the step size, with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix, the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process. To the best of our knowledge, our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

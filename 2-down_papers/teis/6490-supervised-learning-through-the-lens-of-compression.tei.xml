<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On statistical learning via the lens of compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>David</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics Technion</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science Technion -Israel Institute of Technology</orgName>
								<orgName type="department" key="dep4">Department of Mathematics Technion -Israel Institute of Technology</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Moran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics Technion</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science Technion -Israel Institute of Technology</orgName>
								<orgName type="department" key="dep4">Department of Mathematics Technion -Israel Institute of Technology</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Yehudayoff</surname></persName>
							<email>amir.yehudayoff@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics Technion</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science Technion -Israel Institute of Technology</orgName>
								<orgName type="department" key="dep4">Department of Mathematics Technion -Israel Institute of Technology</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On statistical learning via the lens of compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. The central theme of this work is establishing equivalences between learnability and compressibility, and utilizing these equivalences in the study of statistical learning theory. We begin with the setting of multiclass categorization (zero/one loss). We prove that in this case learnability is equivalent to compression of logarithmic sample size, and that uniform convergence implies compression of constant size. We then consider Vapnik&apos;s general learning setting: we show that in order to extend the compressibility-learnability equivalence to this case, it is necessary to consider an approximate variant of compression. Finally, we provide some applications of the compressibility-learnability equivalences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Near-Minimax Optimal Classification with Dyadic Classification Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
							<email>cscott@rice.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Rice University Houston</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin Madison</orgName>
								<address>
									<postCode>77005, 53706</postCode>
									<region>TX, WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nowak</surname></persName>
							<email>nowak@engr.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Rice University Houston</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin Madison</orgName>
								<address>
									<postCode>77005, 53706</postCode>
									<region>TX, WI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Near-Minimax Optimal Classification with Dyadic Classification Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper reports on a family of computationally practical classifiers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classifiers are based on dyadic classification trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) fitting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classification rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n âˆ’1/2 , the parametric rate. We are not aware of any other practical classi-fiers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

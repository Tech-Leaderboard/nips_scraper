<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Dropout and the Local Reparameterization Trick</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
							<email>D.P.Kingma@uva.nl, salimans.tim@gmail.com, M.Welling@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Machine Learning Group</orgName>
								<orgName type="laboratory" key="lab2">Irvine, and the Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Machine Learning Group</orgName>
								<orgName type="laboratory" key="lab2">Irvine, and the Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Machine Learning Group</orgName>
								<orgName type="laboratory" key="lab2">Irvine, and the Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â‡¥</forename><surname>Algoritmica</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Machine Learning Group</orgName>
								<orgName type="laboratory" key="lab2">Irvine, and the Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Dropout and the Local Reparameterization Trick</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local repa-rameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the mini-batch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

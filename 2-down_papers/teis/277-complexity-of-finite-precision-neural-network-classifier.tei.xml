<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complexity of Finite Precision Neural Network Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><surname>Dembo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inform. Systems Lab</orgName>
								<orgName type="laboratory">Inform. Systems Lab. Stanford University Stanford, Calif. 94305</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Calif</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailath</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Inform. Systems Lab</orgName>
								<orgName type="laboratory">Inform. Systems Lab. Stanford University Stanford, Calif. 94305</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Calif</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Dembo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inform. Systems Lab</orgName>
								<orgName type="laboratory">Inform. Systems Lab. Stanford University Stanford, Calif. 94305</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Calif</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Yeung</forename><surname>Siu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inform. Systems Lab</orgName>
								<orgName type="laboratory">Inform. Systems Lab. Stanford University Stanford, Calif. 94305</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Calif</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Complexity of Finite Precision Neural Network Classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>668</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Thomas Kailath Inform. Systems Lab. Stanford University Stanford, Calif. 94305 A rigorous analysis on the finite precision computational &lt;)Spects of neural network as a pattern classifier via a probabilistic approach is presented. Even though there exist negative results on the capability of perceptron, we show the following positive results: Given n pattern vectors each represented by en bits where e &gt; 1, that are uniformly distributed, with high probability the perceptron can perform all possible binary classifications of the patterns. Moreover , the resulting neural network requires a vanishingly small proportion O(log n/n) of the memory that would be required for complete storage of the patterns. Further, the perceptron algorithm takes O(n2) arithmetic operations with high probability, whereas other methods such as linear programming takes O(n 3. 5) in the worst case. We also indicate some mathematical connections with VLSI circuit testing and the theory of random matrices.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

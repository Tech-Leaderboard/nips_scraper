<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Analysis of Turbo Decoding with Gaussian Densities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paat</forename><surname>Rusmevichientong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Analysis of Turbo Decoding with Gaussian Densities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We provide an analysis of the turbo decoding algorithm (TDA) in a setting involving Gaussian densities. In this context, we are able to show that the algorithm converges and that-somewhat surprisingly-though the density generated by the TDA may differ significantly from the desired posterior density, the means of these two densities coincide. 1 Introd uction In many applications, the state of a system must be inferred from noisy observations. Examples include digital communications, speech recognition, and control with incomplete information. Unfortunately, problems of inference are often intractable, and one must resort to approximation methods. One approximate inference method that has recently generated spectacular success in certain coding applications is the turbo decoding algorithm [1, 2], which bears a close resemblance to message-passing algorithms developed in the coding community a few decades ago [4]. It has been shown that the TDA is also related to well-understood exact inference algorithms [5, 6], but its performance on the intractable problems to which it is applied has not been explained through this connection. Several other papers have further developed an understanding of the turbo decoding algorithm. The exact inference algorithms to which turbo decoding has been related are variants of belief propagation [7J. However, this algorithm is designed for inference problems for which graphical models describing conditional independencies form trees, whereas graphical models associated with turbo decoding possess many loops. To understand the behavior of belief propagation in the presence of loops, Weiss has analyzed the algorithm for cases where only a single loop is present [11]. Other analyses that have shed significant light on the performance of the TDA in its original coding context include [8, 9, 10]. In this paper, we develop a new line of analysis for a restrictive setting in which underlying distributions are Gaussian. In this context, inference problems are tractable and the use of approximation algorithms such as the TDA are unnecessary. However , studying the TDA in this context enables a streamlined analysis that generates new insights into its behavior. In particular, we will show that the algorithm converges and that the mean of the resulting distribution coincides with that of the</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

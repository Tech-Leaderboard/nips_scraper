<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Reparameterization Gradient Variance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Brain and Princeton University</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<orgName type="institution" key="instit3">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Foti</surname></persName>
							<email>nfoti@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Brain and Princeton University</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<orgName type="institution" key="instit3">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">D &amp;apos;</forename><surname>Amour</surname></persName>
							<email>alexdamour@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Brain and Princeton University</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<orgName type="institution" key="instit3">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Brain and Princeton University</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<orgName type="institution" key="instit3">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Reparameterization Gradient Variance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the &quot;reparameterization trick,&quot; represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estima-tors are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to generate more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on a non-conjugate hierarchical model and a Bayesian neural net where our method attained orders of magnitude (20-2,000Ã—) reduction in gradient variance resulting in faster and more stable optimization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

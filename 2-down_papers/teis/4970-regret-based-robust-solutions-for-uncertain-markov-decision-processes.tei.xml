<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regret based Robust Solutions for Uncertain Markov Decision Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asrar</forename><surname>Ahmed</surname></persName>
							<email>masrara@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore Management University</orgName>
								<orgName type="institution" key="instit2">Singapore Management University</orgName>
								<orgName type="institution" key="instit3">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit4">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Varakantham</surname></persName>
							<email>pradeepv@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore Management University</orgName>
								<orgName type="institution" key="instit2">Singapore Management University</orgName>
								<orgName type="institution" key="instit3">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit4">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossiri</forename><surname>Adulyasak</surname></persName>
							<email>yossiri@smart.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore Management University</orgName>
								<orgName type="institution" key="instit2">Singapore Management University</orgName>
								<orgName type="institution" key="instit3">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit4">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jaillet</surname></persName>
							<email>jaillet@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore Management University</orgName>
								<orgName type="institution" key="instit2">Singapore Management University</orgName>
								<orgName type="institution" key="instit3">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit4">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Regret based Robust Solutions for Uncertain Markov Decision Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of maximin policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed minimax regret as a suitable alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only. We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

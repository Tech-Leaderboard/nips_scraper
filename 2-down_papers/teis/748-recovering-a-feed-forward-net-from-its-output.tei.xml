<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recovering a Feed-Forward Net From Its Output</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Fefferman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">David Sarnoff Research Center CN5300 Princeton</orgName>
								<orgName type="department" key="dep2">Dept. of Mathematics. Princeton University</orgName>
								<address>
									<postCode>08543-5300, 08544-1000</postCode>
									<settlement>Princeton</settlement>
									<region>N J, NJ</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Markel</surname></persName>
							<email>smarkel@sarnoff.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">David Sarnoff Research Center CN5300 Princeton</orgName>
								<orgName type="department" key="dep2">Dept. of Mathematics. Princeton University</orgName>
								<address>
									<postCode>08543-5300, 08544-1000</postCode>
									<settlement>Princeton</settlement>
									<region>N J, NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recovering a Feed-Forward Net From Its Output</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Alternate address: 335</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study feed-forward nets with arbitrarily many layers, using the standard sigmoid, tanh x. Aside from technicalities, our theorems are: 1. Complete knowledge of the output of a neural net for arbitrary inputs uniquely specifies the architecture, weights and thresholds; and 2. There are only finitely many critical points on the error surface for a generic training problem. Neural nets were originally introduced as highly simplified models of the nervous system. Today they are widely used in technology and studied theoretically by scientists from several disciplines. However, they remain little understood. Mathematically, a (feed-forward) neural net consists of: (1) A finite sequence of positive integers (Do, D 1 , ... , D£); (2) A family of real numbers (wJ d defined for 1 :5 e 5: L, 1 5: j 5: Dl , 1 5: k :5 Dl-l ; and (3) A family of real numbers (OJ) defined for 15: f 5: L, 15: j 5: Dl. The sequence (Do, D1 , .. &quot; DL) is called the architecture of the neural net, while the W]k are called weights and the OJ thresholds. Neural nets are used to compute non-linear maps from }R.N to }R.M by the following construction. vVe begin by fixing a nonlinear function 0-(x) of one variable. Analogy with the nervous system suggests that we take o-(x) asymptotic to constants as x tends to ±oo; a standard choice, which we adopt throughout this paper, is o-(.r) =</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

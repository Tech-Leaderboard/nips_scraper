<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complexity Issues in Neural Computation and Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Roychowdhnry</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Dept.. of Electrical &amp; Compo Engr. U ni versit.y of California at Irvine Irvine</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907, 92717</postCode>
									<region>IN, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Sin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Dept.. of Electrical &amp; Compo Engr. U ni versit.y of California at Irvine Irvine</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907, 92717</postCode>
									<region>IN, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Complexity Issues in Neural Computation and Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The general goal of this workshop was to bring t.ogether researchers working toward developing a theoretical framework for the analysis and design of neural networks. The t.echnical focus of the workshop was to address recent. developments in understanding the capabilities and limitations of variolls modds for neural computation and learning. The primary topics addressed the following three areas: 1) Computational complexity issues in neural networks, 2) Complexity issues in learning, and 3) Convergence and numerical properties of learning algorit.hms. Other topics included experiment.al/simulat.ion results on neural llet.works, which seemed to pose some open problems in the areas of learning and generalizat.ion properties of feedforward networks. The presentat.ions and discussions at the workshop highlighted the int.erdisciplinary nature of research in neural net.works. For example, several of the present.at.ions discussed recent contributions which have applied complexity-theoretic techniques to characterize the computing power of neural net.works, t.o design efficient neural networks, and t.o compare the computational capabilit.ies of neural net.works wit.h that. of convent.ional models for comput.ation. Such st.udies, in t.urn, have generated considerable research interest. among computer scient.ists, as evidenced by a significant number of research publications on related topics. A similar development can be observed in t.he area of learning as well: Techniques primarily developed in the classical theory of learning are being applied to understand t.he generalization and learning characteristics of neural networks. In [1, 2] attempts have been made to integrate concept.s from different areas and present a unifie(i treatment of the various results on the complexity of neural computation ancllearning. In fact, contributions from several part.icipants in the workshop are included in [2], and interested readers could find det.ailed discussions of many of the n-~sults IHesented at t.he workshop in [2]. Following is a brief descriptioll of the present.ations, along with the Hames and e-mail addresses of the speakers. W. Maass (maa.~.~@igi. tu-gTÂ·(Jz.(!(&quot;. at) and A. Sakurai (sakllmi@hadgw92.lwd.hitachi.co.,ip) made preseutatiol1s Oll tlw VC-dimension and t.he comput.ational power of feedforwarcl neural net.works. Many neural net.s of depth 3 (or larger) with linear threshold gat.es have a VC-dimf&apos;usion t.hat. is superlinear in t.he number of weights of the net. The talks presPllted llPW results which establish 1161</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Number of Linear Regions of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Montúfar</surname></persName>
							<email>montufar@mis.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Mathematics in the Sciences</orgName>
								<orgName type="institution" key="instit1">Razvan Pascanu</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<orgName type="institution" key="instit3">Université de Montréal</orgName>
								<orgName type="institution" key="instit4">Université de Montréal</orgName>
								<address>
									<country>CIFAR Fellow</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Mathematics in the Sciences</orgName>
								<orgName type="institution" key="instit1">Razvan Pascanu</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<orgName type="institution" key="instit3">Université de Montréal</orgName>
								<orgName type="institution" key="instit4">Université de Montréal</orgName>
								<address>
									<country>CIFAR Fellow</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Mathematics in the Sciences</orgName>
								<orgName type="institution" key="instit1">Razvan Pascanu</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<orgName type="institution" key="instit3">Université de Montréal</orgName>
								<orgName type="institution" key="instit4">Université de Montréal</orgName>
								<address>
									<country>CIFAR Fellow</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Number of Linear Regions of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>neural network</term>
					<term>input space partition</term>
					<term>rectifier</term>
					<term>maxout</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer&apos;s input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network&apos;s depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

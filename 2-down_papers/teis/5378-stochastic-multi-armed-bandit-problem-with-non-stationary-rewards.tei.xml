<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Besbes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University New York</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit3">Columbia University New York</orgName>
								<address>
									<region>NY, CA, NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Gur</surname></persName>
							<email>ygur@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University New York</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit3">Columbia University New York</orgName>
								<address>
									<region>NY, CA, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zeevi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University New York</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit3">Columbia University New York</orgName>
								<address>
									<region>NY, CA, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler&apos;s objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward &quot;variation&quot; and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

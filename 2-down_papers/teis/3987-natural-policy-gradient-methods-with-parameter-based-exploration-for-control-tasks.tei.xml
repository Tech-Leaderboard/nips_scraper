<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Miyamae</surname></persName>
							<email>{miyamae@fe., nagata@fe., isao@, kobayasi@}dis.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Intelligence and Systems Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Nagata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Intelligence and Systems Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Ono</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Intelligence and Systems Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigenobu</forename><surname>Kobayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Intelligence and Systems Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>â€¡: Research Fellow of the Japan Society for the Promotion of Science</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose an efficient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

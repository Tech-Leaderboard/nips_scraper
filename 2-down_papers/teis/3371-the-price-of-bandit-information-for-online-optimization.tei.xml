<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Price of Bandit Information for Online Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Dani</surname></persName>
							<email>varsha@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago Chicago</orgName>
								<orgName type="institution" key="instit2">Toyota Technological Institute Chicago</orgName>
								<orgName type="institution" key="instit3">Toyota Technological Institute Chicago</orgName>
								<address>
									<postCode>60637, 60637, 60637</postCode>
									<region>IL, IL, IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Hayes</surname></persName>
							<email>hayest@tti-c.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago Chicago</orgName>
								<orgName type="institution" key="instit2">Toyota Technological Institute Chicago</orgName>
								<orgName type="institution" key="instit3">Toyota Technological Institute Chicago</orgName>
								<address>
									<postCode>60637, 60637, 60637</postCode>
									<region>IL, IL, IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago Chicago</orgName>
								<orgName type="institution" key="instit2">Toyota Technological Institute Chicago</orgName>
								<orgName type="institution" key="instit3">Toyota Technological Institute Chicago</orgName>
								<address>
									<postCode>60637, 60637, 60637</postCode>
									<region>IL, IL, IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Price of Bandit Information for Online Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ R n in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full information case, the upper bound on the regret is O * (√ nT), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm which achieves O * (n 3/2 √ T) regret-all previous (nontrivial) bounds here were O(poly(n)T 2/3) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case-in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is exponential (√ T K vs. √ T log K). We also present lower bounds showing that this gap is at least √ n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efficiently in special cases of particular interest, such as path planning and Markov Decision Problems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

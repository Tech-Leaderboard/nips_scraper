<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neurally Plausible Reinforcement Learning of Working Memory Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaldert</forename><forename type="middle">O</forename><surname>Rombouts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Life Sciences Amsterdam</orgName>
								<orgName type="department" key="dep2">Institute for Neuroscience Amsterdam</orgName>
								<orgName type="institution">CWI</orgName>
								<address>
									<country>The Netherlands, Netherlands, The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><forename type="middle">M</forename><surname>Bohte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Life Sciences Amsterdam</orgName>
								<orgName type="department" key="dep2">Institute for Neuroscience Amsterdam</orgName>
								<orgName type="institution">CWI</orgName>
								<address>
									<country>The Netherlands, Netherlands, The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
							<email>p.r.roelfsema@nin.knaw.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Life Sciences Amsterdam</orgName>
								<orgName type="department" key="dep2">Institute for Neuroscience Amsterdam</orgName>
								<orgName type="institution">CWI</orgName>
								<address>
									<country>The Netherlands, Netherlands, The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neurally Plausible Reinforcement Learning of Working Memory Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: by learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity [1]. It is however not well known how such neurons acquire these task-relevant working memories. Here we introduce a biologically plausible learning scheme grounded in Reinforcement Learning (RL) theory [2] that explains how neurons become selective for relevant information by trial and error learning. The model has memory units which learn useful internal state representations to solve working memory tasks by transforming partially observable Markov decision problems (POMDP) into MDPs. We propose that synaptic plasticity is guided by a combination of attentional feedback signals from the action selection stage to earlier processing levels and a globally released neuromodula-tory signal. Feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. The neuromodulatory signal interacts with tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to 1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks [1, 3, 4] and 2) learn to optimally integrate probabilistic evidence for perceptual decision making [5, 6].</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

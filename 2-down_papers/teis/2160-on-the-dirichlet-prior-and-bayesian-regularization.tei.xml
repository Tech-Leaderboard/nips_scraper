<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Dirichlet Prior and Bayesian Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
							<email>harald@ai.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Artificial Intelligence Laboratory</orgName>
								<orgName type="laboratory" key="lab2">Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139, 02139</postCode>
									<region>MA, MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Artificial Intelligence Laboratory</orgName>
								<orgName type="laboratory" key="lab2">Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139, 02139</postCode>
									<region>MA, MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Dirichlet Prior and Bayesian Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A common objective in learning a model from data is to recover its network structure, while the model parameters are of minor interest. For example, we may wish to recover regulatory networks from high-throughput data sources. In this paper we examine how Bayesian regularization using a product of independent Dirichlet priors over the model parameters affects the learned model structure in a domain with discrete variables. We show that a small scale parameter-often interpreted as &quot;equivalent sample size&quot; or &quot;prior strength&quot;-leads to a strong regularization of the model structure (sparse graph) given a sufficiently large data set. In particular , the empty graph is obtained in the limit of a vanishing scale parameter. This is diametrically opposite to what one may expect in this limit, namely the complete graph from an (unregularized) maximum likelihood estimate. Since the prior affects the parameters as expected, the scale parameter balances a trade-off between regularizing the parameters vs. the structure of the model. We demonstrate the benefits of optimizing this trade-off in the sense of predictive accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divisive Normalization, Line Attractor Networks and Ideal Observers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Deneve</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pougetl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">2Dpt of Neurobiology</orgName>
								<orgName type="institution" key="instit1">Georgetown Institute for Computational and Cognitive Sciences</orgName>
								<orgName type="institution" key="instit2">Georgetown University</orgName>
								<address>
									<postCode>20007-2197</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UCLA</orgName>
								<address>
									<postCode>90095-1763</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Divisive Normalization, Line Attractor Networks and Ideal Observers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Gain control by divisive inhibition, a.k.a. divisive normalization, has been proposed to be a general mechanism throughout the visual cortex. We explore in this study the statistical properties of this normalization in the presence of noise. Using simulations, we show that divisive normalization is a close approximation to a maximum likelihood estimator, which, in the context of population coding, is the same as an ideal observer. We also demonstrate analytically that this is a general property of a large class of nonlinear recurrent networks with line attractors. Our work suggests that divisive normalization plays a critical role in noise filtering, and that every cortical layer may be an ideal observer of the activity in the preceding layer. Information processing in the cortex is often formalized as a sequence of a linear stages followed by a nonlinearity. In the visual cortex, the nonlinearity is best described by squaring combined with a divisive pooling of local activities. The divisive part of the nonlinearity has been extensively studied by Heeger and colleagues [1], and several authors have explored the role of this normalization in the computation of high order visual features such as orientation of edges or first and second order motion[ 4]. We show in this paper that divisive normalization can also playa role in noise filtering. More specifically, we demonstrate through simulations that networks implementing this normalization come close to performing maximum likelihood estimation. We then demonstrate analytically that the ability to perform maximum likelihood estimation, and thus efficiently extract information from a population of noisy neurons, is a property exhibited by a large class of networks. Maximum likelihood estimation is a framework commonly used in the theory of ideal observers. A recent example comes from the work of Itti et al., 1998, who have shown that it is possible to account for the behavior of human subjects in simple discrimination tasks. Their model comprised two distinct stages: 1) a network</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

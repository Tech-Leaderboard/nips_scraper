<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selecting the State-Representation in Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odalric-Ambrym</forename><surname>Maillard</surname></persName>
							<email>odalricambrym.maillard@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Lille -Nord Europe</orgName>
								<orgName type="institution" key="instit2">INRIA Lille -Nord Europe</orgName>
								<orgName type="institution" key="instit3">INRIA Lille -Nord Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©mi</forename><surname>Munos</surname></persName>
							<email>remi.munos@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Lille -Nord Europe</orgName>
								<orgName type="institution" key="instit2">INRIA Lille -Nord Europe</orgName>
								<orgName type="institution" key="instit3">INRIA Lille -Nord Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Ryabko</surname></persName>
							<email>daniil@ryabko.net</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Lille -Nord Europe</orgName>
								<orgName type="institution" key="instit2">INRIA Lille -Nord Europe</orgName>
								<orgName type="institution" key="instit3">INRIA Lille -Nord Europe</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Selecting the State-Representation in Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T 2/3 where T is the horizon time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

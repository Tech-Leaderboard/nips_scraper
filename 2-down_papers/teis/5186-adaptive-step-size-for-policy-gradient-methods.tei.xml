<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Step-Size for Policy Gradient Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pirotta</surname></persName>
							<email>matteo.pirotta@polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Elect., Inf., and Bio</orgName>
								<orgName type="department" key="dep2">Dept. Elect., Inf</orgName>
								<orgName type="department" key="dep3">Dept. Elect., Inf</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country>ITALY, ITALY, ITALY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Restelli</surname></persName>
							<email>marcello.restelli@polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Elect., Inf., and Bio</orgName>
								<orgName type="department" key="dep2">Dept. Elect., Inf</orgName>
								<orgName type="department" key="dep3">Dept. Elect., Inf</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country>ITALY, ITALY, ITALY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bascetta</surname></persName>
							<email>luca.bascetta@polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Elect., Inf., and Bio</orgName>
								<orgName type="department" key="dep2">Dept. Elect., Inf</orgName>
								<orgName type="department" key="dep3">Dept. Elect., Inf</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country>ITALY, ITALY, ITALY</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Step-Size for Policy Gradient Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the last decade, policy gradient methods have significantly grown in popularity in the reinforcement-learning field. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction , since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step-size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second-order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear-quadratic regulator problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reward Mapping for Transfer in Long-Lived Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science and Eng</orgName>
								<orgName type="department" key="dep2">Computer Science and Eng</orgName>
								<orgName type="department" key="dep3">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science and Eng</orgName>
								<orgName type="department" key="dep2">Computer Science and Eng</orgName>
								<orgName type="department" key="dep3">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science and Eng</orgName>
								<orgName type="department" key="dep2">Computer Science and Eng</orgName>
								<orgName type="department" key="dep3">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reward Mapping for Transfer in Long-Lived Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a finite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent&apos;s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Specifically, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent&apos;s performance relative to other approaches, including an approach that transfers policies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

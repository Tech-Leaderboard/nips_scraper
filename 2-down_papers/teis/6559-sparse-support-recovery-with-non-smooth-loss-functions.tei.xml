<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Support Recovery with Non-smooth Loss Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kévin</forename><surname>Degraux</surname></persName>
							<email>kevin.degraux@uclouvain.be</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
							<email>gabriel.peyre@ens.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jalal</forename><forename type="middle">M</forename><surname>Fadili</surname></persName>
							<email>Jalal.Fadili@ensicaen.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Jacques</surname></persName>
							<email>laurent.jacques@uclouvain.be</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">ISPGroup/ICTEAM</orgName>
								<orgName type="institution">FNRS Université catholique de Louvain Louvain-la-Neuve</orgName>
								<address>
									<postCode>1348</postCode>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">DMA École Normale Supérieure Paris</orgName>
								<address>
									<postCode>75775</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">ISPGroup/ICTEAM, FNRS Université catholique de Louvain Louvain-la-Neuve</orgName>
								<orgName type="institution" key="instit1">Normandie Univ</orgName>
								<orgName type="institution" key="instit2">ENSICAEN</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>14050, 1348</postCode>
									<settlement>Caen</settlement>
									<country>France, Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Support Recovery with Non-smooth Loss Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we study the support recovery guarantees of underdetermined sparse regression using the 1-norm as a regularizer and a non-smooth loss function for data fidelity. More precisely, we focus in detail on the cases of 1 and ∞ losses, and contrast them with the usual 2 loss. While these losses are routinely used to account for either sparse (1 loss) or uniform (∞ loss) noise models, a theoretical analysis of their performance is still lacking. In this article, we extend the existing theory from the smooth 2 case to these non-smooth cases. We derive a sharp condition which ensures that the support of the vector to recover is stable to small additive noise in the observations, as long as the loss constraint size is tuned proportionally to the noise level. A distinctive feature of our theory is that it also explains what happens when the support is unstable. While the support is not stable anymore, we identify an &quot;extended support&quot; and show that this extended support is stable to small additive noise. To exemplify the usefulness of our theory, we give a detailed numerical analysis of the support stability/instability of compressed sensing recovery with these different losses. This highlights different parameter regimes, ranging from total support stability to progressively increasing support instability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/melina/Documents/js/scrape/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-02-21T06:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel Dimensionality Reduction for Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
							<email>fukumizu@ism.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Statistical Mathematics Tokyo</orgName>
								<orgName type="department" key="dep2">CS Division</orgName>
								<orgName type="laboratory">CS Division and Statistics University of California Berkeley</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<postCode>106-8569, 94720, 94720</postCode>
									<region>CA, CA</region>
									<country>Japan, USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
							<email>fbach@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Statistical Mathematics Tokyo</orgName>
								<orgName type="department" key="dep2">CS Division</orgName>
								<orgName type="laboratory">CS Division and Statistics University of California Berkeley</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<postCode>106-8569, 94720, 94720</postCode>
									<region>CA, CA</region>
									<country>Japan, USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Statistical Mathematics Tokyo</orgName>
								<orgName type="department" key="dep2">CS Division</orgName>
								<orgName type="laboratory">CS Division and Statistics University of California Berkeley</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<postCode>106-8569, 94720, 94720</postCode>
									<region>CA, CA</region>
									<country>Japan, USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kernel Dimensionality Reduction for Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel method of dimensionality reduction for supervised learning. Given a regression or classification problem in which we wish to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of finding a low-dimensional &quot;ef-fective subspace&quot; of X which retains the statistical relationship between X and Y. We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem, we characterize the notion of conditional independence using covariance operators on reproducing kernel Hilbert spaces; this allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>
